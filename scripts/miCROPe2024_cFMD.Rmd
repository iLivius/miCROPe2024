---
title: "Harnessing machine learning for predictive tools in microbiome studies: a focus on amplicon sequence variants and metagenomic shotgun sequencing"
subtitle: "Second case study: cFMD v1.1.0 - curated Food Microbiome Data repository"
author: 
- "Livio Antonielli"
- "Niccol√≤ Carlino"
- "Nicola Segata"
date: "April 4, 2025"
eeditor_options: 
  chunk_output_type: console
format: html
editor: visual
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

#### Setup PATH

```{r setting wd, echo = FALSE, message = FALSE, warning = FALSE}
setwd("~/miCROPe2024/")
```

#### Install and source libraries

```{r sourcing pkgs, echo=FALSE, message=FALSE, warning=FALSE}
# List of packages to be installed from GitHub
github_packages <- c("mlr-org/mlr3extralearners@*release", "mlr-org/mlr3proba")

# Function to check if a package is installed
is_installed <- function(pkg) is.element(pkg, installed.packages()[, "Package"])

# Install 'devtools' package if not installed
if (!is_installed("devtools")) {
  install.packages("devtools")
}

# Load 'devtools' package
library(devtools)

# Install 'remotes' package if not installed
if (!is_installed("remotes")) {
  install.packages("remotes")
}

# Load 'remotes' package
library(remotes)

# Install packages from GitHub
for (pkg in github_packages) {
  if (!is_installed(pkg)) {
    remotes::install_github(pkg)
  }
}

# Load libraries
if (!require("pacman"))
  install.packages("pacman")
pacman::p_load(
  callr,
  corrplot,
  cowplot,
  data.table,
  future,
  future.apply,
  gridExtra,
  iml,
  magrittr,
  mikropml,
  mlr3extralearners,
  mlr3hyperband,
  mlr3proba,
  mlr3verse,
  mlr3tuning,
  mlr3tuningspaces,
  mlr3viz,
  OpenML,
  patchwork,
  parallel,
  paradox,
  psych,
  SMOTEWB,
  tidyheatmaps,
  tidyverse,
  wordcloud,
  wordcloud2,
  install = TRUE
)
```

#### Import data

```{r import, echo=FALSE, message=FALSE, warning=FALSE}
# Path to local tar.gz file
archive_path <- "data/cFMD_taxonomic_profiles.tar.gz"

# Extract the TSV file from the archive to a temporary location
untar(archive_path, exdir = tempdir())

# Build the extracted file path
tsv_path <- file.path(tempdir(), "cFMD_taxonomic_profiles.tsv")

# Import the TSV file
tab <- read.table(tsv_path, header = TRUE, row.names = 1, sep = "\t")

# Transpose table
tab_pre <- as.data.frame(t(tab))

# Assign original row names as column names
colnames(tab_pre) <- rownames(tab)
```

#### Set parameters

```{r selecting dataset, echo = FALSE, message = FALSE, warning = FALSE}
# Reduce verbosity
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

# Set seed for reproducibility
seed = 42
set.seed(seed)

# Set number of CPUs
num_threads = detectCores()/2

# Increase the maximum size allowed for future.apply workers to 2 GB
options(future.globals.maxSize = 2 * 1024^3)

# Time for an algorithm to run before a terminator will kill the tuning instance, in minutes
term_min = 30

# Define a variable to use as target. Use either category, type, or subtype.
target <- "category"
```

#### Visualize data

```{r visualize, echo = FALSE, message = FALSE, warning = FALSE}
# Count the number of observations for each category level
category_counts <- tab_pre %>%
  group_by(category) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(category = str_replace_all(category, "_", " "))

# Create a lollipop chart for the number of observations in each category
lolli_1 <- ggplot(category_counts, aes(x = reorder(category, count), y = count)) +
  geom_segment(aes(xend = category, yend = 0),
               color = "steelblue",
               linewidth = 1.2) +
  geom_point(color = "steelblue", size = 12) +
  geom_text(
    aes(label = count),
    color = "white",
    fontface = "bold",
    size = 3
  ) +
  theme_minimal() +
  coord_flip() +
  labs(title = "Number of samples for each category", x = target, y = "Count") +
  theme(
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# Count the number of distinct type levels within each category
types_per_category <- tab_pre %>%
  group_by(category) %>%
  summarise(type_count = n_distinct(type)) %>%
  ungroup() %>%
  mutate(category = str_replace_all(category, "_", " "))

# Create a lollipop chart for the number of types within each category
lolli_2 <- ggplot(types_per_category, aes(x = reorder(category, type_count), y = type_count)) +
  geom_segment(aes(xend = category, yend = 0),
               color = "darkorange",
               linewidth = 1.2) +
  geom_point(color = "darkorange", size = 12) +
  geom_text(
    aes(label = type_count),
    color = "white",
    fontface = "bold",
    size = 3
  ) +
  theme_minimal() +
  coord_flip() +
  labs(title = "Number of types within each category", x = NULL, y = "Number of types") +
  theme(
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# combined lolli plot
lolli_plot <- plot_grid(lolli_1, lolli_2, ncol = 2, rel_widths = c(1, 1))
lolli_plot

# Filter the data for the dairy category
dairy_data <- tab_pre %>%
  filter(category == "dairy") %>%
  mutate(type = str_replace_all(type, "_", " "))

# Count the number of observations for each dairy type
dairy_counts <- dairy_data %>%
  group_by(type) %>%
  summarise(count = n()) %>%
  ungroup()

# Create a barplot with the number of observations for each dairy type
dairy_plot <- ggplot(dairy_counts, aes(x = reorder(type, count), y = count)) +
  geom_bar(stat = "identity", fill = "darkcyan") +
  geom_text(aes(label = count), vjust = -0.5, size = 4) +
  theme_minimal() +
  labs(title = "Number of samples for each dairy type", x = "Dairy type", y = "Count") +
  theme(
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1, vjust = 1),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

print(dairy_plot)

# Filter the data for the dairy category and cheese type
cheese_data <- tab_pre %>%
  filter(category == "dairy" & type == "cheese") %>%
  mutate(subtype = str_replace_all(subtype, "_", " "))

# Count the number of observations for each cheese subtype
cheese_counts <- cheese_data %>%
  group_by(subtype) %>%
  summarise(count = n()) %>%
  ungroup()

# Prepare data for the word cloud
cheese_wordcloud_data <- cheese_counts %>%
  rename(word = subtype, freq = count)

# Filter the cheese_wordcloud_data to include only the specified varieties
cheese_wordcloud_data <- cheese_counts %>%
  rename(word = subtype, freq = count)

# Create the word cloud
wordcloud(
  words = cheese_wordcloud_data$word,
  freq = cheese_wordcloud_data$freq,
  min.freq = 1,
  max.words = 200,
  random.order = FALSE,
  rot.per = 0.35,
  scale = c(2.5, 0.25),
  colors = brewer.pal(8, "Dark2")
)
#wordcloud2(cheese_wordcloud_data, size = 0.7, color = 'random-dark')
```

#### Pre-process data

```{r pre-process, echo = FALSE, message = FALSE, warning = FALSE}
#####################################################
# Feature selection and minority class oversampling #
#####################################################

# Convert character columns as factors and keep only microbial variables and target factor
tab_sel <- tab_pre %>% mutate(across(starts_with("k__"), as.numeric)) %>% 
  mutate(across(where(is_character), as_factor)) %>% 
  select(!!as.symbol(target), starts_with("k__"))

# Rename the target column as "target"
colnames(tab_sel)[which(names(tab_sel) == target)] <- "target"

# Remove rows corresponding to classes with < 10 observations
tab_sel %<>%
  group_by(target) %>%
  mutate(n = n()) %>%
  filter(n >= 10) %>%
  select(-n) %>%
  as.data.frame() %>%
  droplevels.data.frame()

# Define if the dataset is suitable for either binary or multi-class classification
# as.data.table(mlr_measures)
positive_class = ""

if (length(levels(as.factor(tab_sel$target))) > 2) {
  dataset_attr = "multi-class"
  measures <- c("classif.acc", "classif.bacc", "classif.logloss")
  measure <- measures[2]
} else if (length(levels(as.factor(tab_sel$target))) == 2) {
  dataset_attr = "binary"
  measures <-
    c(
      "classif.acc",
      "classif.bacc",
      "classif.logloss",
      "classif.precision",
      "classif.recall",
      "classif.specificity",
      "classif.fbeta",
      "classif.auc",
      "classif.prauc"
    )
  measure <- measures[2]
  if (positive_class == "") {
    # Calculate the counts of each class
    counts <- table(tab_sel$target)
    # Find the minority class
    minority_class <- names(which.min(counts))
    positive_class <- minority_class
  }
} else {
  cat("The target column is neither binary nor multi-class. Please, check!")
}

# Print the type of data and the classification measures used
cat("\n", paste("The dataset is suitable for", dataset_attr, "classification and available measures are:\n"))
cat(paste(measures, collapse = "\n"), "\n")
cat(paste("The measure selected to display the performance is:", measure), "\n")

# Re-shuffling samples randomly
tab_sel %<>% rename_with(~ gsub("\\|", "_", .))
tab_sel %<>% sample_frac(size = 1)

# Scaling and collapsing (nearly) perfectly correlated variables
tab_pro <- preprocess_data(
  tab_sel,
  outcome_colname = "target",
  remove_var = "nzv",
  collapse_corr_feats = TRUE,
  to_numeric = TRUE,
  group_neg_corr = TRUE,
  prefilter_threshold = 1
)$dat_transformed

# Balance the dataset by oversampling minority classes
smote_list <- SMOTE(x = subset(tab_pro, select = -target) , y = tab_pro$target, k = 5)
tab_train <- cbind(as.data.frame(smote_list$x_new), target = smote_list$y_new)

# Create a new table for building plots
temp_data <- tab_train

# Function to parse taxon names
parse_taxon_name <- function(name) {
  parsed_name <- str_extract(name, "(?<=_s__).*")
  parsed_name <- str_replace_all(parsed_name, "_t__", "_")
  return(parsed_name)
}

# Identify the position of the "target" column
target_col <- which(colnames(temp_data) == "target")

# Simplify column names, excluding the "target" column
taxon_cols <- setdiff(1:ncol(temp_data), target_col)
colnames(temp_data)[taxon_cols] <- sapply(colnames(temp_data)[taxon_cols], parse_taxon_name)

# Convert to tidy format, keeping "target" intact
temp_tidy_data <- temp_data %>%
  pivot_longer(-target, names_to = "taxon", values_to = "abundance")

# Summarize duplicates by taking the mean abundance
temp_mean_data <- temp_tidy_data %>%
  group_by(taxon, target) %>%
  summarise(abundance = mean(abundance, na.rm = TRUE), .groups = 'drop')

# Generate heatmap
tidyheatmap(
  df = temp_mean_data,
  rows = taxon,
  columns = target,
  values = abundance,
  cluster_rows = TRUE,
  cluster_cols = TRUE,
  angle_col = 45
)

# Reshape to wide format for correlation
temp_wide_data <- temp_mean_data %>%
  pivot_wider(names_from = taxon, values_from = abundance) %>%
  column_to_rownames("target") %>%
  as.data.frame()

# Function to parse taxon names to keep everything starting with "SGB" or "EUK"
parse_taxon_name <- function(name) {
  parsed_name <- str_extract(name, "(SGB\\d+.*|EUK\\d+.*)")
  return(parsed_name)
}

# Simplify column names, applying to all columns (since "target" is now a row name)
colnames(temp_wide_data) <- sapply(colnames(temp_wide_data), parse_taxon_name)

# Compute correlation
corr <- corr.test(temp_wide_data,
                  adjust = "fdr",
                  method = "spearman")

# Generate correlation plot
corrplot(
  as.matrix(corr$r),
  p.mat = as.matrix(corr$p),
  sig.level = 0.05,
  insig = 'blank',
  order = 'hclust',
  method = 'pie',
  col.lim = c(-1, 1),
  col = colorRampPalette(c("blue", "white", "red"))(200)
)
```

#### Train models

```{r classification, echo=FALSE, message=FALSE, warning=FALSE}
####################################################################
# Tune model hyperparameters via inner cross-validation resampling #
####################################################################

# Create a classification task
task <- TaskClassif$new(id = "cFMD",
                        backend = tab_train,
                        target = "target")

# Stratify resampling
task$set_col_roles("target", c("target", "stratum"))

# Provide a positive class to task
if (dataset_attr == "binary") {
  task$positive <- positive_class
}

# Choose methods
methods <- list(lda = "Linear Discriminant Analysis",
                ranger = "Random Forest",
                xgboost = "XGBoost")

# Make learners
learners <- list(
  lda = lrn("classif.lda", predict_type = "prob"),
  ranger = lrn("classif.ranger", predict_type = "prob"),
  xgboost = lrn("classif.xgboost", predict_type = "prob")
)

# Set the number of threads for each learner
for (learner in learners) {
  learner <- set_threads(learner, n = num_threads)
}

# XGBoost metrics
if (dataset_attr == "binary") {
  eval_metrics = c("error", "logloss")
} else if (dataset_attr == "multi-class") {
  eval_metrics = c("merror", "mlogloss")
}

# Set planned operations sequentially
future::plan(sequential)

# Resampling setting
kfold = 10
repeats = 10

# Count the observation in each target group
min_count <-
  tab_train %>% count(target) %>% arrange(n) %>% slice_head(n = 1) %>% select(n) %>% as.numeric()

# Loop through all methods
for (i in 1:length(methods)) {
  # Print the start time
  start_time <- Sys.time()
  cat("\n", "##### Method: ", methods[[i]], " ######", "\n")
  cat(" Start time: ", format(start_time, "%Y-%m-%d %H:%M:%S"), "\n")
  
  method = names(methods[i])
  learner <- learners[[method]]
  
  # Define resampling strategy
  if (min_count <= kfold) {
    inner_resampling <- rsmp("bootstrap", repeats = repeats, ratio = 1)
  } else if (method == "xgboost") {
    inner_resampling <- rsmp("cv", folds = kfold)
  } else {
    inner_resampling <-
      rsmp("repeated_cv", folds = kfold, repeats = repeats)
  }
  
  # Set up hyperparameters
  if (method == "ranger") {
    learner$param_set$set_values(
      num.trees = to_tune(p_int(10, 1000, tags = "budget")),
      mtry.ratio = to_tune(p_dbl(0, 1)),
      sample.fraction = to_tune(p_dbl(0.1, 1))
    )
  } else if (method == "xgboost") {
    learner$param_set$set_values(
      nrounds = to_tune(p_int(10, 1000, tags = "budget")),
      eta = to_tune(1e-4, 1, logscale = TRUE),
      max_depth = to_tune(1, 20),
      colsample_bytree = to_tune(1e-1, 1),
      colsample_bylevel = to_tune(1e-1, 1),
      lambda = to_tune(1e-3, 1e3, logscale = TRUE),
      alpha = to_tune(1e-3, 1e3, logscale = TRUE),
      subsample = to_tune(1e-1, 1),
      eval_metric = eval_metrics
    )
  }
  # Train the learner on the entire dataset
  if (method == "lda") {
    learner$train(task)
  } else {
    if (method == "ranger") {
      # Define a hyperband tuner
      tuner <- tnr("hyperband", eta = 3, repetitions = 1)
      # Define the terminator
      terminator <- trm("run_time", secs = 60 * term_min)
    } else if (method == "xgboost") {
      tuner <- tnr("hyperband", eta = 3, repetitions = 1)
      # Define the terminator
      terminator <- trm("none")
    }
    # Create a tuning instance
    instance <- ti(
      task = task,
      learner = learner,
      resampling = inner_resampling,
      measure = msr(measure),
      terminator = terminator
    )
    # Tuning hp
    tuner$optimize(instance)
    # Set learner with best hp
    learner$param_set$values <- instance$result_learner_param_vals
    # Train the learner
    learner$train(task)
    # Save the instance
    assign(paste(method, "tuned", "instance", target, sep = "_"),
           instance)
    # Save the best model
    assign(paste(method, "tuned", "learner", target, sep = "_"), learner)
  }
  
  # Print the end time
  end_time <- Sys.time()
  cat(" End time: ", format(end_time, "%Y-%m-%d %H:%M:%S"), "\n")
  
  # Calculate and print the time difference
  time_diff <- difftime(end_time, start_time, units = "secs")
  if (time_diff < 60) {
    cat(" Time taken for algorithm ",
        method,
        ": ",
        time_diff,
        " seconds\n")
  } else {
    time_diff <- difftime(end_time, start_time, units = "mins")
    cat(" Time taken for algorithm ",
        method,
        ": ",
        time_diff,
        " minutes\n")
  }
}
```

#### Benchmark

```{r benchmarking, echo = FALSE, message = FALSE, warning = FALSE}
##########################################################
# Benchmark multiple learners via outer cross-validation #
##########################################################

# Set the number of threads for each learner back to 1
for (learner in learners) {
  learner <- set_threads(learner, n = 1)
}

# Shall we use all threads?
future::plan(multisession, workers = num_threads)

# Resampling strategy
if (min_count <= kfold) {
  outer_resampling <- rsmp("bootstrap", repeats = repeats, ratio = 1)
} else {
  outer_resampling <- rsmp("cv", folds = kfold)
}

# Make a list of learners from methods available in "methods", tuned with best hyper-parameters
learners_best <- list()
for (i in 1:length(methods)) {
  method = names(methods[i])
  learner <- learners[[method]]
  if (method == "lda") {
    # Add the learner without tuning to the list
    learners_best[[method]] <- learner
  } else {
    # Get the best model
    best_model <- get(paste(method, "tuned", "learner", target, sep = "_"))
    # Add the best model to the list
    learners_best[[method]] <- best_model
  }
}

# Print the start time
start_time <- Sys.time()
cat(" Start time: ", format(start_time, "%Y-%m-%d %H:%M:%S"), "\n")

# Benchmark the learners
bmr <-
  benchmark(design = benchmark_grid(task, learners_best, outer_resampling))

# Print the end time
end_time <- Sys.time()
cat(" End time: ", format(end_time, "%Y-%m-%d %H:%M:%S"), "\n")

# Calculate and print the time difference
time_diff <- difftime(end_time, start_time, units = "secs")
if (time_diff < 60) {
  cat(" Time taken for benchmarking:", time_diff, " seconds\n")
} else {
  time_diff <- difftime(end_time, start_time, units = "mins")
  cat(" Time taken for benchmarking: ", time_diff, " minutes\n")
}

# Boxplots of trained models and accuracy
bench_boxplot <- autoplot(bmr, measure = msr(measure)) +
  labs(x = "Method", y = paste(measure, "measure")) +
  theme_bw() +
  scale_x_discrete(labels = as.character(unlist(methods))) +
   labs(title = paste0(toupper(substr(target, 1, 1)), substr(target, 2, nchar(target)), " benchmark"),
       y = "Balanced accuracy") +
  theme(
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18, hjust = 0.5),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# Convert the benchmark results to a data frame
bench_df <- as.data.frame(bmr$aggregate(measures = lapply(measures, msr)))

# Add a new column for the method name
bench_df %<>%
  mutate(
    method = case_when(
      str_detect(learner_id, "lda") ~ "Linear Discriminant Analysis",
      str_detect(learner_id, "ranger") ~ "Random Forest",
      str_detect(learner_id, "xgboost") ~ "XGBoost",
      TRUE ~ "Other"
    )
  ) %>%
  relocate(tail(names(.), 1), .before = names(.)[1]) %>%
  select(-nr, -resample_result, -task_id, -resampling_id, -iters) %>%
  rename_with(~ case_when(
    .x == "classif.acc" ~ "accuracy",
    .x == "classif.bacc" ~ "balanced accuracy",
    .x == "classif.precision" ~ "precision",
    .x == "classif.recall" ~ "recall",
    .x == "classif.specificity" ~ "specificity",
    .x == "classif.fbeta" ~ "f1",
    .x == "classif.auc" ~ "auc",
    .x == "classif.prauc" ~ "pr auc",
    .x == "classif.logloss" ~ "logloss",
    TRUE ~ .x  # keep the original name if no condition is met
  ))

# Print the method name and accuracy
bench_df %>%
  select(-learner_id) %>%
  print()

best_method <- bench_df %>%
  dplyr::arrange(desc(`balanced accuracy`)) %>%
  dplyr::select(learner_id) %>%
  slice_head(n = 1) %>%
  as.character()

# Prediction outcome table
method <-
  str_split(best_method, pattern = "\\.", simplify = TRUE)[[2]]
method_name <-
  methods[[str_split(best_method, pattern = "\\.", simplify = TRUE)[2]]]
cat("\n",
    paste("Model selected for", target, "prediction based on:", method_name, sep = " "),
    "\n")
learner <- learners[[method]]
if (method == "lda") {
  # Train the learner
  learner$train(task)
  # Make a prediction using the learner without tuning
  pred <- learner$predict(task)
} else {
  # Get the best model
  best_model <- get(paste(method, "tuned", "learner", target, sep = "_"))
  # Make a prediction using the best model
  pred <- best_model$predict(task)
}
# Print the prediction accuracy
cat(paste(
  "\n",
  measure,
  "of",
  method_name,
  ":",
  round(pred$score(msr(measure)), 2),
  "\n",
  sep = " "
))

# Get the truth, response and probabilities
truth <- pred$truth
response <- pred$response
prob <- pred$prob
# Create a data frame
pred_df <- data.frame(truth = truth, response = response, prob)
# Add a column to check if the prediction matches the truth
pred_df$match <-
  ifelse(pred_df$truth == pred_df$response, "TRUE", "FALSE")
# Save predictions on train data
assign(paste(method, "train", "predict", target, sep = "_"), pred_df)
# Print number of correctly and incorrectly classified observations
print(pred_df %>%
        count(match) %>%
        mutate(ratio = round(n / sum(n), digits = 2)))
# Print number of correctly and incorrectly classified observations for each label of the target
print(
  pred_df %>%
    group_by(truth) %>%
    summarize(total = n(), correct = sum(match == TRUE)) %>%
    mutate(accuracy = correct / total)
)

# Create the dodged barplot for accuracy, balanced accuracy, and logloss with custom colors
bench_long <- bench_df %>%
  select(method, accuracy, `balanced accuracy`, logloss) %>%
  pivot_longer(cols = c(accuracy, `balanced accuracy`, logloss), names_to = "measure", values_to = "value")

# Define the color palette
metric_colors <- c("accuracy" = "skyblue3", "balanced accuracy" = "dodgerblue3", "logloss" = "tomato3")

bench_barplot <- ggplot(bench_long, aes(x = method, y = value, fill = measure)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = metric_colors) +
  geom_text(aes(label = round(value, 2)), 
            position = position_dodge(width = 0.9), 
            vjust = 1.5, 
            color = "white", 
            size = 3) +
  theme_minimal() +
  labs(title = "Model performance measures",
       x = "Method",
       y = "Value") +
  theme(
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# Create the barplot for pred_df values
bench_pred_df <- pred_df %>%
  group_by(truth) %>%
  summarize(total = n(), correct = sum(match == TRUE)) %>%
  mutate(accuracy = correct / total)

bench_pred_barplot <- ggplot(bench_pred_df, aes(x = reorder(truth, accuracy), y = accuracy)) +
  geom_bar(stat = "identity", fill = "darkcyan") +
  geom_text(aes(label = round(accuracy, 2)), hjust = -0.3, size = 4) +
  geom_text(aes(label = paste(correct, "/", total)), y = 0.05, color = "white", size = 4, hjust = 0) +
  theme_minimal() +
  labs(title = paste("Accuracy by", target, sep = " "),
       x = target,
       y = "Accuracy") +
  coord_flip() +
  theme(
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# Combine the three plots using cowplot
bench_comb_plot <- plot_grid(
  bench_boxplot,
  plot_grid(bench_barplot, bench_pred_barplot, ncol = 1, rel_heights = c(1, 1)),
  ncol = 2,
  rel_widths = c(1, 1)
)

# Print the combined plot
print(bench_comb_plot)

#################################################################
# Evaluate the benchmark across different resampling iterations #
#################################################################

# Extract predictions from each fold
pred_list <- lapply(bmr$score()$prediction_test, as.data.table)
all_pred_dt <- rbindlist(pred_list, idcol = "fold")

# Compute accuracy per class
all_pred_dt[, correct := truth == response]
class_errors <- all_pred_dt[, .(
    correct = sum(correct),
    incorrect = .N - sum(correct),
    accuracy = sum(correct) / .N
), by = truth]
print(class_errors)

# Identify misclassified samples
misclassified_samples <- all_pred_dt[correct == FALSE]
print(misclassified_samples)

# Generate barplots for absolute and relative predictions
pred_summary <- all_pred_dt[, .N, by = .(truth, correct)]
pred_summary[, prop := N / sum(N), by = truth]

p1 <- ggplot(pred_summary, aes(x = truth, y = N, fill = correct, label = N)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("TRUE" = "dodgerblue3", "FALSE" = "red3")) +
  geom_text(position = position_stack(vjust = 0.5), color = "white") +
  theme_minimal() +
  labs(title = "Absolute Counts of True & False Predictions",
       x = "Target Class",
       y = "Count",
       fill = "Prediction Correct") +
  labs(title = "Relative Composition of Predictions",
       x = "Target Class",
       y = "Proportion",
       fill = "Prediction Correct") +
  theme(
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18, hjust = 0.5),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

p2 <- ggplot(pred_summary,aes(x = truth, y = prop, fill = correct, label = sprintf("%.0f%%", prop * 100))) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("TRUE" = "dodgerblue3", "FALSE" = "red3")) +
  geom_text(position = position_stack(vjust = 0.5), color = "white") +
  theme_minimal() +
  labs(title = "Relative Composition of Predictions",
       x = "Target Class",
       y = "Proportion",
       fill = "Prediction Correct") +
  theme(
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18, hjust = 0.5),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# Jitter plot for true vs predicted labels
p3 <- ggplot(all_pred_dt, aes(x = truth, y = response, color = correct)) +
  geom_jitter(alpha = 0.3,
              width = 0.2,
              height = 0.2) +
  scale_color_manual(values = c("TRUE" = "dodgerblue3", "FALSE" = "red3")) +
  theme_minimal() +
  labs(
    title = "True vs. Predicted Labels",
    x = "True Label",
    y = "Predicted Label",
    color = "Correct Prediction"
  ) +
  theme(
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    plot.title = element_text(size = 18, hjust = 0.5),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 14)
  )

# Arrange plots in a grid
p1 + p2 + p3 + plot_layout(ncol = 3)
```

#### Feature importance

```{r importance, echo = FALSE, message = FALSE, warning = FALSE}
# Define the function to parse taxon names
parse_taxon_name <- function(name) {
  parsed_name <- str_extract(name, "(?<=_s__).*")
  parsed_name <- str_replace_all(parsed_name, "_t__", "_")
  return(parsed_name)
}

if (target %in% c("category", "type")) {
  # Create a mapping between original and simplified names
  temp_tab <- tab_train %>% select(-target) %>% as.data.frame()
  original_names <- colnames(temp_tab)
  simplified_names <- sapply(original_names, parse_taxon_name)
  
  name_mapping <- setNames(simplified_names, original_names)
  
  # Train the model and calculate feature importance
  if (method == "lda") {
    # Train the learner
    learner$train(task)
    # Build R6 object on selected training model
    temp_pred = Predictor$new(model = learner, data = temp_tab, y = tab_train$target, type = "prob")
  } else {
    # Get the best model
    best_model <- get(paste(method, "tuned", "learner", target, sep = "_"))
    # Build R6 object on selected training model
    temp_pred = Predictor$new(model = best_model, data = temp_tab, y = tab_train$target, type = "prob")
  }
  
  # Calculate feature importance
  future::plan(future::multisession, workers = num_threads / 2)
  feat_imp = FeatureImp$new(temp_pred, loss = "ce", n.repetitions = 100, compare = "difference")
  
  # Get feature importance results
  feat_imp_results <- feat_imp$results
  
  # Modify the feature importance results to use simplified names
  feat_imp_results$feature <- name_mapping[feat_imp_results$feature]
  
  # Remove any NA values resulting from unmatched names
  feat_imp_results <- feat_imp_results[!is.na(feat_imp_results$feature), ]
  
  # Create the ggplot2 plot
  feat_imp_plot <- ggplot(feat_imp_results, aes(x = reorder(feature, importance), y = importance)) +
    geom_bar(stat = "identity", fill = "mediumseagreen") +
    geom_errorbar(aes(ymin = importance.05, ymax = importance.95), width = 0.4) +
    coord_flip() +
    labs(title = "Feature importance", x = "Feature", y = "Importance (cross entropy loss)") +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 10),
      plot.title = element_text(size = 14, face = "bold"),
      axis.title = element_text(size = 12)
    )
  
  # Print the modified plot
  print(feat_imp_plot)
}
```
